{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfolder_path = \"/kaggle/input/ai-vs-human-generated-dataset/train_data\"\n\n\"\"\"for root, dirs, files in os.walk(folder_path):\n    for file in files:\n        print(os.path.join(root, file))\"\"\"  # Dosyanın tam yolunu yazdır","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:23:38.102774Z","iopub.execute_input":"2025-02-02T15:23:38.103073Z","iopub.status.idle":"2025-02-02T15:23:39.073822Z","shell.execute_reply.started":"2025-02-02T15:23:38.103042Z","shell.execute_reply":"2025-02-02T15:23:39.072978Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'for root, dirs, files in os.walk(folder_path):\\n    for file in files:\\n        print(os.path.join(root, file))'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation, RandomResizedCrop\nfrom torch.utils.data import DataLoader, random_split\nimport torch\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:23:39.074626Z","iopub.execute_input":"2025-02-02T15:23:39.075086Z","iopub.status.idle":"2025-02-02T15:23:59.728003Z","shell.execute_reply.started":"2025-02-02T15:23:39.075056Z","shell.execute_reply":"2025-02-02T15:23:59.727194Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"csv_file = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\ndata = pd.read_csv(csv_file)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:23:59.728845Z","iopub.execute_input":"2025-02-02T15:23:59.729493Z","iopub.status.idle":"2025-02-02T15:23:59.893270Z","shell.execute_reply.started":"2025-02-02T15:23:59.729460Z","shell.execute_reply":"2025-02-02T15:23:59.892317Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0                                        file_name  label\n0               0  train_data/a6dcb93f596a43249135678dfcfc17ea.jpg      1\n1               1  train_data/041be3153810433ab146bc97d5af505c.jpg      0\n2               2  train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg      1\n3               3  train_data/8542fe161d9147be8e835e50c0de39cd.jpg      0\n4               4  train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg      1\n...           ...                                              ...    ...\n79945       79945  train_data/9283b107f6274279b6f15bbe77c523aa.jpg      0\n79946       79946  train_data/4c6b17fe6dd743428a45773135a10508.jpg      1\n79947       79947  train_data/1ccbf96d04e342fd9f629ad55466b29e.jpg      0\n79948       79948  train_data/ff960b55f296445abb3c5f304b52e104.jpg      1\n79949       79949  train_data/3abd1876472f4ec988aa78f76664fbd6.jpg      0\n\n[79950 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>file_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>train_data/a6dcb93f596a43249135678dfcfc17ea.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>train_data/041be3153810433ab146bc97d5af505c.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>train_data/8542fe161d9147be8e835e50c0de39cd.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>79945</th>\n      <td>79945</td>\n      <td>train_data/9283b107f6274279b6f15bbe77c523aa.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>79946</th>\n      <td>79946</td>\n      <td>train_data/4c6b17fe6dd743428a45773135a10508.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>79947</th>\n      <td>79947</td>\n      <td>train_data/1ccbf96d04e342fd9f629ad55466b29e.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>79948</th>\n      <td>79948</td>\n      <td>train_data/ff960b55f296445abb3c5f304b52e104.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>79949</th>\n      <td>79949</td>\n      <td>train_data/3abd1876472f4ec988aa78f76664fbd6.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>79950 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\"\"\"import os\nimport shutil\nimport pandas as pd\n\n# Dosyaların bulunduğu ana klasör\nsource_folder = \"/kaggle/input/ai-vs-human-generated-dataset/\"\n\n# Hedef ana klasör\ndestination_root = \"/kaggle/working/sorted\"\n\n# CSV dosyasının yolu\ncsv_file = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\n\n# Klasörleri oluştur ve dosyaları kopyala\ndef organize_files():\n    # CSV dosyasını pandas ile oku\n    data = pd.read_csv(csv_file)\n\n    for _, row in data.iterrows():\n        file_name = row['file_name']\n        label = str(row[\"label\"])\n        source_path = os.path.join(source_folder, file_name)\n        destination_folder = os.path.join(destination_root, label)\n\n        # Hedef klasörü oluştur\n        os.makedirs(destination_folder, exist_ok=True)\n        \n        # Dosyayı kopyala\n        if os.path.exists(source_path):\n            shutil.copy(source_path, destination_folder)\n            print(f\"{file_name} kopyalandı -> {label}\")\n        else:\n            print(f\"Hata: {file_name} bulunamadı.\")\n\norganize_files()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:23:59.894264Z","iopub.execute_input":"2025-02-02T15:23:59.894626Z","iopub.status.idle":"2025-02-02T15:23:59.900117Z","shell.execute_reply.started":"2025-02-02T15:23:59.894585Z","shell.execute_reply":"2025-02-02T15:23:59.899198Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'import os\\nimport shutil\\nimport pandas as pd\\n\\n# Dosyaların bulunduğu ana klasör\\nsource_folder = \"/kaggle/input/ai-vs-human-generated-dataset/\"\\n\\n# Hedef ana klasör\\ndestination_root = \"/kaggle/working/sorted\"\\n\\n# CSV dosyasının yolu\\ncsv_file = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\\n\\n# Klasörleri oluştur ve dosyaları kopyala\\ndef organize_files():\\n    # CSV dosyasını pandas ile oku\\n    data = pd.read_csv(csv_file)\\n\\n    for _, row in data.iterrows():\\n        file_name = row[\\'file_name\\']\\n        label = str(row[\"label\"])\\n        source_path = os.path.join(source_folder, file_name)\\n        destination_folder = os.path.join(destination_root, label)\\n\\n        # Hedef klasörü oluştur\\n        os.makedirs(destination_folder, exist_ok=True)\\n        \\n        # Dosyayı kopyala\\n        if os.path.exists(source_path):\\n            shutil.copy(source_path, destination_folder)\\n            print(f\"{file_name} kopyalandı -> {label}\")\\n        else:\\n            print(f\"Hata: {file_name} bulunamadı.\")\\n\\norganize_files()'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass MultiLabelDataset(Dataset):\n  def __init__(self, root, df, transform,is_train=True):\n    self.root = root\n    self.df = df\n    self.transform = transform\n    self.is_train=is_train\n  def __getitem__(self, idx):\n    item = self.df.iloc[idx]\n    # get image\n    image_path = os.path.join(self.root, item[\"file_name\"])\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # prepare image for the model\n    pixel_values = self.transform(image)\n\n    # get labels\n    label = torch.tensor(self.df.iloc[idx]['label'])\n   \n    # turn into PyTorch te\n    return pixel_values, label\n        # For inference, return only the image\n    \n  def __len__(self):\n    return len(self.df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:23:59.901087Z","iopub.execute_input":"2025-02-02T15:23:59.901362Z","iopub.status.idle":"2025-02-02T15:23:59.920060Z","shell.execute_reply.started":"2025-02-02T15:23:59.901339Z","shell.execute_reply":"2025-02-02T15:23:59.919185Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_csv_path = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\ntrain_df = pd.read_csv(train_csv_path)\n\n# Define the base directory where images are stored\ntrain_data_dir = \"/kaggle/input/ai-vs-human-generated-dataset/\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:23:59.922573Z","iopub.execute_input":"2025-02-02T15:23:59.922833Z","iopub.status.idle":"2025-02-02T15:24:00.013899Z","shell.execute_reply.started":"2025-02-02T15:23:59.922810Z","shell.execute_reply":"2025-02-02T15:24:00.013020Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"labels=np.unique(train_df['label'].astype(\"string\"))\nid2label = {k: v for k, v in enumerate(labels)}\nlabel2id = {v: k for k, v in enumerate(labels)}\nprint(id2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:00.015379Z","iopub.execute_input":"2025-02-02T15:24:00.015689Z","iopub.status.idle":"2025-02-02T15:24:00.094272Z","shell.execute_reply.started":"2025-02-02T15:24:00.015659Z","shell.execute_reply":"2025-02-02T15:24:00.093476Z"}},"outputs":[{"name":"stdout","text":"{0: '0', 1: '1'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:00.095295Z","iopub.execute_input":"2025-02-02T15:24:00.095583Z","iopub.status.idle":"2025-02-02T15:24:00.122154Z","shell.execute_reply.started":"2025-02-02T15:24:00.095558Z","shell.execute_reply":"2025-02-02T15:24:00.121451Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\nnormalize = Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:00.122876Z","iopub.execute_input":"2025-02-02T15:24:00.123154Z","iopub.status.idle":"2025-02-02T15:24:02.392608Z","shell.execute_reply.started":"2025-02-02T15:24:00.123129Z","shell.execute_reply":"2025-02-02T15:24:02.391838Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"068b6fb4ddd845f9a948229a49288963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef2e16f05a64418a753687510130f98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70710abe93454d74ae334592cb94ae21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbee4589ebef40d09c2717e022516ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68a5ae4ae764ea4b1a2ee1cf1c04855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1378e7f9989f42ff90f87e96da874565"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"SEED=42\ntrain_transform = Compose([\n    Resize((336, 336)),\n    RandomHorizontalFlip(),\n    \n    \n    ToTensor(),\n    normalize\n])\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED, stratify=train_df['label'])\n\n# Dataset and DataLoader\ntrain_dataset = MultiLabelDataset(train_data_dir,train_df,  transform=train_transform, is_train=True)\nval_dataset = MultiLabelDataset( train_data_dir,val_df, transform=train_transform, is_train=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:02.393421Z","iopub.execute_input":"2025-02-02T15:24:02.393629Z","iopub.status.idle":"2025-02-02T15:24:02.431399Z","shell.execute_reply.started":"2025-02-02T15:24:02.393613Z","shell.execute_reply":"2025-02-02T15:24:02.430725Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    data = torch.stack([item[0] for item in batch])\n    target = torch.stack([item[1] for item in batch])\n    return data, target\n\ntrain_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=8, shuffle=True)\nvalid_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:02.432145Z","iopub.execute_input":"2025-02-02T15:24:02.432348Z","iopub.status.idle":"2025-02-02T15:24:02.437185Z","shell.execute_reply.started":"2025-02-02T15:24:02.432331Z","shell.execute_reply":"2025-02-02T15:24:02.436362Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\nmodel.config.id2label = id2label\nmodel.config.label2id = label2id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:02.438155Z","iopub.execute_input":"2025-02-02T15:24:02.438444Z","iopub.status.idle":"2025-02-02T15:24:11.577345Z","shell.execute_reply.started":"2025-02-02T15:24:02.438422Z","shell.execute_reply":"2025-02-02T15:24:11.575863Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da05f09005bb4ffd81f1fb5b2c4ab07b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3928b4571cec45c5b3d656325639e64b"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"visual_encoder = model.vision_model\n\n# Use DataParallel for multi-GPU training\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    visual_encoder = torch.nn.DataParallel(visual_encoder)\n\nvisual_encoder.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:11.578784Z","iopub.execute_input":"2025-02-02T15:24:11.579095Z","iopub.status.idle":"2025-02-02T15:24:12.172569Z","shell.execute_reply.started":"2025-02-02T15:24:11.579068Z","shell.execute_reply":"2025-02-02T15:24:12.171785Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CLIPVisionTransformer(\n  (embeddings): CLIPVisionEmbeddings(\n    (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n    (position_embedding): Embedding(577, 1024)\n  )\n  (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  (encoder): CLIPEncoder(\n    (layers): ModuleList(\n      (0-23): 24 x CLIPEncoderLayer(\n        (self_attn): CLIPSdpaAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): QuickGELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.AdamW(visual_encoder.parameters(), lr=0.00001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n\ntrain_losses = []\nval_losses = []\naccumulation_steps = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:12.173399Z","iopub.execute_input":"2025-02-02T15:24:12.173623Z","iopub.status.idle":"2025-02-02T15:24:12.180058Z","shell.execute_reply.started":"2025-02-02T15:24:12.173593Z","shell.execute_reply":"2025-02-02T15:24:12.179061Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def predict_test(model, test_csv_path, test_images_dir, id2label, device, output_csv):\n    test_data = pd.read_csv(test_csv_path)\n    test_data[\"label\"] = \"NaN\"\n\n    model.eval()\n    with torch.no_grad():\n        for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n            image_path = os.path.join(test_images_dir, row[\"file_name\"])\n            if not os.path.exists(image_path):\n                continue\n\n            # Preprocess the image\n            image = Image.open(image_path).convert(\"RGB\")\n            image_tensor = transform(image).unsqueeze(0).to(device)\n\n            # Predict\n            outputs = model(pixel_values=image_tensor)\n            features = outputs.pooler_output\n            predicted_class = torch.argmax(features, dim=-1).item()\n            city_name = id2label[predicted_class]\n            test_data.at[idx, \"label\"] = city_name\n\n    # Save predictions\n    test_data.to_csv(output_csv, index=False)\n    print(f\"Test predictions saved to {output_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:12.180738Z","iopub.execute_input":"2025-02-02T15:24:12.180938Z","iopub.status.idle":"2025-02-02T15:24:12.195940Z","shell.execute_reply.started":"2025-02-02T15:24:12.180921Z","shell.execute_reply":"2025-02-02T15:24:12.195159Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for epoch in range(4):\n    print(\"Epoch:\", epoch)\n    visual_encoder.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    optimizer.zero_grad()\n    for idx, (images,labels) in enumerate(tqdm(train_dataloader)):\n        images, labels = images.to(device), labels.to(device)\n        \n        outputs = visual_encoder(images)\n        logits = outputs.pooler_output  # Extract pooled output for classification\n        loss = criterion(logits,labels) / accumulation_steps\n        accelerator.backward(loss)\n        \n        \n        if (idx + 1) % accumulation_steps == 0 or (idx + 1) == len(train_dataloader):\n            optimizer.step()\n            optimizer.zero_grad()\n\n        running_loss += loss.item() * accumulation_steps\n        total += labels.shape[0]\n        predicted = logits.argmax(-1)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(train_dataloader)\n    train_losses.append(train_loss)\n    train_accuracy = correct / total\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}\")\n\n    # Validation\n    visual_encoder.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for val_batch in val_dataloader:\n            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n            val_outputs = visual_encoder(pixel_values=val_batch[\"pixel_values\"])\n            val_logits = val_outputs.pooler_output\n            val_loss = criterion(val_logits, val_batch[\"labels\"])\n            val_running_loss += val_loss.item()\n            val_total += val_batch[\"labels\"].shape[0]\n            val_predicted = val_logits.argmax(-1)\n            val_correct += (val_predicted == val_batch[\"labels\"]).sum().item()\n\n    val_loss = val_running_loss / len(val_dataloader)\n    val_losses.append(val_loss)\n    val_accuracy = val_correct / val_total\n    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n\n    scheduler.step(val_loss)\n\n    # Save the model\n    model_path = f\"clip_visual_model_epoch_{epoch}.pth\"\n    torch.save(visual_encoder.state_dict(), model_path)\n    print(f\"Model saved to {model_path}\")\n\n    # Test predictions\n    test_csv_path = \"/kaggle/input/ai-vs-human-generated-dataset/test.csv\"\n    test_images_dir = \"/kaggle/input/ai-vs-human-generated-dataset/test_data_v2\"\n    submission_path = f\"submission_epoch_{epoch}.csv\"\n    predict_test(visual_encoder, test_csv_path, test_images_dir, id2label, device, submission_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T15:24:12.196806Z","iopub.execute_input":"2025-02-02T15:24:12.197148Z","iopub.status.idle":"2025-02-02T15:25:46.996581Z","shell.execute_reply.started":"2025-02-02T15:24:12.197116Z","shell.execute_reply":"2025-02-02T15:25:46.995361Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 52/7995 [01:34<4:00:28,  1.82s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a784f9800631>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-b6209d184cf6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# get image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# prepare image for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ai-vs-human-generated-dataset/train_data/f41820666d1847d791bd2da3326d2838.jpg'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/ai-vs-human-generated-dataset/train_data/f41820666d1847d791bd2da3326d2838.jpg'","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}